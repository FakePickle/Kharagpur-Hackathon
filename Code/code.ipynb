{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (0.11.5)\n",
      "Requirement already satisfied: sentence_transformers in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: click in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from pdfplumber) (11.1.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (44.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from sentence_transformers) (4.47.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from sentence_transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from sentence_transformers) (1.15.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from sentence_transformers) (0.27.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.2.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.12.14)\n",
      "Requirement already satisfied: pycparser in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk pdfplumber sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Text from the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leveraging Clustering Techniques for Enhanced\n",
      "Drone Monitoring and Position Estimation\n",
      "Abstract\n",
      "Drone tracking and localization are essential for various applications, including\n",
      "managing drone formations and implementing anti-drone strategies. Pinpointing\n",
      "and monitoring drones in three-dimensional space is difficult, particularly when\n",
      "trying to capture the subtle movements of small drones during rapid maneuvers.\n",
      "This involves extracting faint signals from varied flight settings and maintaining\n",
      "alignment despite swift actions. Typically, cameras and LiDAR systems are used\n",
      "to record the paths of drones. However, they encounter challenges in categorizing\n",
      "drones and estimating their positions accurately. This report provides an overview\n",
      "of an approach named CL-Det. It uses a clustering-based learning detection strategy\n",
      "to track and estimate the position of drones using data from two types of LiDAR\n",
      "sensors: Livox Avia and LiDAR 360. This method merges data from both LiDAR\n",
      "sources to accurately determine the droneâ€™s location in three dimensions. The\n",
      "method begins by synchronizing the time codes of the data from the two sensors\n",
      "and then isolates the point cloud data for the objects of interest (OOIs) from the\n",
      "environmental data. A Density-Based Spatial Clustering of Applications with\n",
      "Noise (DBSCAN) method is applied to cluster the OOI point cloud data, and the\n",
      "center point of the most prominent cluster is taken as the droneâ€™s location. The\n",
      "technique also incorporates past position estimates to compensate for any missing\n",
      "information.\n",
      "1 Introduction\n",
      "Unmanned aerial vehicles (UA Vs), commonly referred to as drones, have gained prominence and\n",
      "significantly influence areas like logistics, imaging, and emergency response, offering substantial\n",
      "advantages to society. However, the broad adoption and sophisticated features of compact, off-the-\n",
      "shelf drones have created intricate security issues that extend beyond conventional risks.\n",
      "Recent years have witnessed a surge in research on anti-UA V systems. Present anti-UA V methods\n",
      "predominantly utilize visual, radar, and radio frequency (RF) technologies. Despite these strides,\n",
      "recognizing drones poses a considerable hurdle for sensors like cameras, particularly when drones\n",
      "are at significant altitudes or in challenging visual environments. These methods usually fail to spot\n",
      "small drones because of their minimal size, which leads to a decreased radar cross-section and a\n",
      "less noticeable visual presence. Furthermore, current anti-UA V studies primarily focus on detecting\n",
      "objects and tracking them in two dimensions, overlooking the crucial element of estimating their\n",
      "3D paths. This omission significantly restricts the effectiveness of anti-UA V systems in practical,\n",
      "real-world contexts.\n",
      "Our proposed solution, a detection method based on clustering learning (CL-Det), uses the strengths\n",
      "of both Livox Avia and LiDAR 360 to improve the tracking and position estimation of UA Vs.\n",
      "Initially, the timestamps from the Livox Avia and LiDAR 360 data are aligned to maintain temporal\n",
      "consistency. By examining the LiDAR data, which contains the spatial coordinates of objects at\n",
      "specific times, and comparing these to the actual recorded positions of the drone at those times, the\n",
      "droneâ€™s location within the LiDAR point cloud data is effectively pinpointed. The point cloud for\n",
      ".\n",
      "objects of interest (OOIs) is then isolated from the environmental data. The point cloud of the OOIs\n",
      "is grouped using the DBSCAN algorithm, and the central point of the largest cluster is designated as\n",
      "the UA Vâ€™s position. Moreover, radar data also faces significant challenges due to missing information.\n",
      "To mitigate potential data deficiencies, past estimations are employed to supplement missing data,\n",
      "thereby maintaining the consistency and precision of UA V tracking.\n",
      "2 Methodology\n",
      "This section details the methodology employed to ascertain the droneâ€™s spatial position utilizing\n",
      "information from LiDAR 360 and Livox Avia sensors. The strategy integrates data from both sensor\n",
      "types to achieve precise position calculations.\n",
      "2.1 Data Sources\n",
      "The following modalities of data were utilized:\n",
      "â€¢ Double fisheye camera visual images\n",
      "â€¢ Livox Mid-360 (LiDAR 360) 3D point cloud data\n",
      "â€¢ Livox Avia 3D point cloud data\n",
      "â€¢ Millimeter-wave radar 3D point cloud data\n",
      "Only 14 out of 59 test sequences have non-zero radar values; therefore, the radar dataset is excluded\n",
      "from this work due to data availability issues. Two primary sensor types are employed: LiDAR 360\n",
      "and Livox Avia, both of which supply 3D point cloud data crucial for identifying the droneâ€™s location.\n",
      "The detailed data descriptions are outlined as follows:\n",
      "â€¢LiDAR 360 offers a complete 360-degree view with 3D point cloud data. This dataset\n",
      "encompasses environmental details and other observable objects.\n",
      "â€¢Livox Avia delivers focused 3D point cloud data at specific timestamps, typically indicating\n",
      "the origin point or the droneâ€™s position.\n",
      "2.2 Algorithm\n",
      "For every sequence, corresponding positions are recorded at specific timestamps. The procedure\n",
      "gives precedence to LiDAR 360 data, using Livox Avia data as a backup if the former is not available.\n",
      "If neither source is accessible, the position is estimated using historical averages.\n",
      "2.2.1 LiDAR 360 Data Processing\n",
      "â€¢Separation of Points: The LiDAR 360 data is visually examined to classify areas into two\n",
      "zones: environment and non-environment zones.\n",
      "â€¢Removal of Environment Points: All points within the environment zone are deemed part\n",
      "of the surroundings and are thus excluded from the dataset. After removing environment\n",
      "points, it is observed that the remaining non-environment points imply the drone position.\n",
      "â€¢Clustering: The DBSCAN clustering algorithm is applied to the remaining points to discern\n",
      "distinct clusters.\n",
      "â€¢Cluster Selection: The most extensive non-environment cluster is chosen as the representa-\n",
      "tive group of points that correspond to the drone.\n",
      "â€¢Mean Position Calculation: The droneâ€™s position is determined by calculating the mean of\n",
      "the selected cluster, represented by (x, y, z) coordinates.\n",
      "2.2.2 Livox Avia Data Processing\n",
      "â€¢Removal of Noise: Points with coordinates (0, 0, 0) are eliminated as they are regarded as\n",
      "noise.\n",
      "â€¢Mean Position Calculation: The mean of the residual points is computed to ascertain the\n",
      "droneâ€™s position in (x, y, z) coordinates.\n",
      "2\n",
      "2.2.3 Fallback Method\n",
      "When neither LiDAR 360 nor Livox Avia data is available, the average location of the drone derived\n",
      "from training datasets is used. The average ground truth position (x, y, z) from all training datasets\n",
      "estimates the drone ground truth position, which is (0.734, -9.739, 33.353).\n",
      "2.3 Implementation Details\n",
      "The program fetches LiDAR 360 or Livox Avia data from the nearest timestamp for each sequence,\n",
      "as indicated in the test dataset. Clustering is executed using the DBSCAN algorithm with appro-\n",
      "priate parameters to guarantee strong clustering. Visual inspection is employed for the preliminary\n",
      "separation of points, ensuring an accurate categorization of environment points.\n",
      "The implementation was conducted on a Lenovo IdeaPad Slim 5 Pro (16\") running Windows 11\n",
      "with an AMD Ryzen 7 5800H CPU and 16GB DDR4 RAM. The analysis was carried out in a\n",
      "Jupyter Notebook environment using Python 3.10. For clustering, the DBSCAN algorithm from the\n",
      "Scikit-Learn library was utilized. The DBSCAN algorithm was configured with an epsilon (eps)\n",
      "value of 2 and a minimum number of points (minPts) set to 1.\n",
      "3 Results\n",
      "The algorithm achieved a pose MSE loss of 120.215 and a classification accuracy of 0.322. Table 1\n",
      "presents the evaluation results compared to other teams.\n",
      "Table 1: Evaluation results on the leaderboard\n",
      "Team ID Pose MSE ( â†“) Accuracy ( â†‘)\n",
      "SDUCZS 58198 2.21375 0.8136\n",
      "Gaofen Lab 57978 7.299575 0.3220\n",
      "sysutlt 57843 24.50694 0.3220\n",
      "casetrous 58233 56.880267 0.2542\n",
      "NTU-ICG (ours) 58268 120.215107 0.3220\n",
      "MTC 58180 189.669428 0.2724\n",
      "gzist 56936 417.396317 0.2302\n",
      "4 Conclusions\n",
      "This paper introduces a clustering-based learning method, CL-Det, which employs advanced cluster-\n",
      "ing techniques such as K-Means and DBSCAN for drone detection and position estimation using\n",
      "LiDAR data. The approach guarantees dependable and precise drone position estimation by utilizing\n",
      "multi-sensor data and robust clustering methods. Fallback mechanisms are in place to ensure con-\n",
      "tinuous position estimation even when primary sensor data is absent. Through thorough parameter\n",
      "optimization and comparative assessment, the proposed methodâ€™s effective performance in drone\n",
      "tracking and position estimation is demonstrated.\n",
      "3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:16: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\vikra\\AppData\\Local\\Temp\\ipykernel_26660\\2564268618.py:16: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  file_path = \"..\\Dataset\\Papers\\P001.pdf\"\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file using PyPDF2.\n",
    "    :param file_path: Path to the PDF file.\n",
    "    :return: Extracted text as a single string.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"  # Extract text page by page\n",
    "    return text\n",
    "\n",
    "file_path = \"..\\Dataset\\Papers\\P001.pdf\"\n",
    "pdf_text = extract_text_from_pdf(file_path)\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title and Abstract Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:\n",
      " Leveraging Clustering Techniques for Enhanced\n",
      "Drone Monitoring and Position Estimation\n",
      "\n",
      "Abstract:\n",
      " Drone tracking and localization are essential for various applications, including\n",
      "managing drone formations and implementing anti-drone strategies. Pinpointing\n",
      "and monitoring drones in three-dimensional space is difficult, particularly when\n",
      "trying to capture the subtle movements of small drones during rapid maneuvers.\n",
      "This involves extracting faint signals from varied flight settings and maintaining\n",
      "alignment despite swift actions. Typically, cameras and LiDAR systems are used\n",
      "to record the paths of drones. However, they encounter challenges in categorizing\n",
      "drones and estimating their positions accurately. This report provides an overview\n",
      "of an approach named CL-Det. It uses a clustering-based learning detection strategy\n",
      "to track and estimate the position of drones using data from two types of LiDAR\n",
      "sensors: Livox Avia and LiDAR 360. This method merges data from both LiDAR\n",
      "sources to accurately determine the droneâ€™s location in three dimensions. The\n",
      "method begins by synchronizing the time codes of the data from the two sensors\n",
      "and then isolates the point cloud data for the objects of interest (OOIs) from the\n",
      "environmental data. A Density-Based Spatial Clustering of Applications with\n",
      "Noise (DBSCAN) method is applied to cluster the OOI point cloud data, and the\n",
      "center point of the most prominent cluster is taken as the droneâ€™s location. The\n",
      "technique also incorporates past position estimates to compensate for any missing\n",
      "information.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Function to dynamically extract title and abstract\n",
    "def extract_title_and_abstract(pdf_text):\n",
    "    \"\"\"\n",
    "    Extract the title and abstract dynamically from the PDF text.\n",
    "    Title: From the start to \"Abstract\".\n",
    "    Abstract: From \"Abstract\" to \"Introduction\".\n",
    "    \"\"\"\n",
    "    title, abstract = \"\", \"\"\n",
    "    try:\n",
    "        # Extract Title\n",
    "        title_end_idx = pdf_text.index(\"Abstract\")\n",
    "        title = pdf_text[:title_end_idx].strip()\n",
    "\n",
    "        # Extract Abstract\n",
    "        abstract_start_idx = pdf_text.index(\"Abstract\") + len(\"Abstract\")\n",
    "        abstract_end_idx = pdf_text.index(\"Introduction\")\n",
    "        abstract = pdf_text[abstract_start_idx:abstract_end_idx].strip()\n",
    "    except ValueError as e:\n",
    "        print(f\"Error extracting title and abstract: {e}\")\n",
    "    \n",
    "    return title, abstract\n",
    "\n",
    "title, abstract = extract_title_and_abstract(pdf_text)\n",
    "\n",
    "print(\"Title:\\n\", title)\n",
    "print(\"\\nAbstract:\\n\", abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Paragraphs with No Full Stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID: True\n",
      "Paragraph: Leveraging Clustering Techniques for Enhanced\n",
      "Drone Monitoring and Position Estimation\n",
      "Abstract\n",
      "Drone tracking and localization are essential for various applications, including\n",
      "managing drone formations and implementing anti-drone strategies. Pinpointing\n",
      "and monitoring drones in three-dimensional space is difficult, particularly when\n",
      "trying to capture the subtle movements of small drones during rapid maneuvers.\n",
      "This involves extracting faint signals from varied flight settings and maintaining\n",
      "alignment despite swift actions. Typically, cameras and LiDAR systems are used\n",
      "to record the paths of drones. However, they encounter challenges in categorizing\n",
      "drones and estimating their positions accurately. This report provides an overview\n",
      "of an approach named CL-Det. It uses a clustering-based learning detection strategy\n",
      "to track and estimate the position of drones using data from two types of LiDAR\n",
      "sensors: Livox Avia and LiDAR 360. This method merges data from both LiDAR\n",
      "sources to accurately determine the droneâ€™s location in three dimensions. The\n",
      "method begins by synchronizing the time codes of the data from the two sensors\n",
      "and then isolates the point cloud data for the objects of interest (OOIs) from the\n",
      "environmental data. A Density-Based Spatial Clustering of Applications with\n",
      "Noise (DBSCAN) method is applied to cluster the OOI point cloud data, and the\n",
      "center point of the most prominent cluster is taken as the droneâ€™s location. The\n",
      "technique also incorporates past position estimates to compensate for any missing\n",
      "information.\n",
      "1 Introduction\n",
      "Unmanned aerial vehicles (UA Vs), commonly referred to as drones, have gained prominence and\n",
      "significantly influence areas like logistics, imaging, and emergency response, offering substantial\n",
      "advantages to society. However, the broad adoption and sophisticated features of compact, off-the-\n",
      "shelf drones have created intricate security issues that extend beyond conventional risks.\n",
      "Recent years have witnessed a surge in research on anti-UA V systems. Present anti-UA V methods\n",
      "predominantly utilize visual, radar, and radio frequency (RF) technologies. Despite these strides,\n",
      "recognizing drones poses a considerable hurdle for sensors like cameras, particularly when drones\n",
      "are at significant altitudes or in challenging visual environments. These methods usually fail to spot\n",
      "small drones because of their minimal size, which leads to a decreased radar cross-section and a\n",
      "less noticeable visual presence. Furthermore, current anti-UA V studies primarily focus on detecting\n",
      "objects and tracking them in two dimensions, overlooking the crucial element of estimating their\n",
      "3D paths. This omission significantly restricts the effectiveness of anti-UA V systems in practical,\n",
      "real-world contexts.\n",
      "Our proposed solution, a detection method based on clustering learning (CL-Det), uses the strengths\n",
      "of both Livox Avia and LiDAR 360 to improve the tracking and position estimation of UA Vs.\n",
      "Initially, the timestamps from the Livox Avia and LiDAR 360 data are aligned to maintain temporal\n",
      "consistency. By examining the LiDAR data, which contains the spatial coordinates of objects at\n",
      "specific times, and comparing these to the actual recorded positions of the drone at those times, the\n",
      "droneâ€™s location within the LiDAR point cloud data is effectively pinpointed. The point cloud for\n",
      ".\n",
      "objects of interest (OOIs) is then isolated from the environmental data. The point cloud of the OOIs\n",
      "is grouped using the DBSCAN algorithm, and the central point of the largest cluster is designated as\n",
      "the UA Vâ€™s position. Moreover, radar data also faces significant challenges due to missing information.\n",
      "To mitigate potential data deficiencies, past estimations are employed to supplement missing data,\n",
      "thereby maintaining the consistency and precision of UA V tracking.\n",
      "2 Methodology\n",
      "This section details the methodology employed to ascertain the droneâ€™s spatial position utilizing\n",
      "information from LiDAR 360 and Livox Avia sensors. The strategy integrates data from both sensor\n",
      "types to achieve precise position calculations.\n",
      "2.1 Data Sources\n",
      "The following modalities of data were utilized:\n",
      "â€¢ Double fisheye camera visual images\n",
      "â€¢ Livox Mid-360 (LiDAR 360) 3D point cloud data\n",
      "â€¢ Livox Avia 3D point cloud data\n",
      "â€¢ Millimeter-wave radar 3D point cloud data\n",
      "Only 14 out of 59 test sequences have non-zero radar values; therefore, the radar dataset is excluded\n",
      "from this work due to data availability issues. Two primary sensor types are employed: LiDAR 360\n",
      "and Livox Avia, both of which supply 3D point cloud data crucial for identifying the droneâ€™s location.\n",
      "The detailed data descriptions are outlined as follows:\n",
      "â€¢LiDAR 360 offers a complete 360-degree view with 3D point cloud data. This dataset\n",
      "encompasses environmental details and other observable objects.\n",
      "â€¢Livox Avia delivers focused 3D point cloud data at specific timestamps, typically indicating\n",
      "the origin point or the droneâ€™s position.\n",
      "2.2 Algorithm\n",
      "For every sequence, corresponding positions are recorded at specific timestamps. The procedure\n",
      "gives precedence to LiDAR 360 data, using Livox Avia data as a backup if the former is not available.\n",
      "If neither source is accessible, the position is estimated using historical averages.\n",
      "2.2.1 LiDAR 360 Data Processing\n",
      "â€¢Separation of Points: The LiDAR 360 data is visually examined to classify areas into two\n",
      "zones: environment and non-environment zones.\n",
      "â€¢Removal of Environment Points: All points within the environment zone are deemed part\n",
      "of the surroundings and are thus excluded from the dataset. After removing environment\n",
      "points, it is observed that the remaining non-environment points imply the drone position.\n",
      "â€¢Clustering: The DBSCAN clustering algorithm is applied to the remaining points to discern\n",
      "distinct clusters.\n",
      "â€¢Cluster Selection: The most extensive non-environment cluster is chosen as the representa-\n",
      "tive group of points that correspond to the drone.\n",
      "â€¢Mean Position Calculation: The droneâ€™s position is determined by calculating the mean of\n",
      "the selected cluster, represented by (x, y, z) coordinates.\n",
      "2.2.2 Livox Avia Data Processing\n",
      "â€¢Removal of Noise: Points with coordinates (0, 0, 0) are eliminated as they are regarded as\n",
      "noise.\n",
      "â€¢Mean Position Calculation: The mean of the residual points is computed to ascertain the\n",
      "droneâ€™s position in (x, y, z) coordinates.\n",
      "2\n",
      "2.2.3 Fallback Method\n",
      "When neither LiDAR 360 nor Livox Avia data is available, the average location of the drone derived\n",
      "from training datasets is used. The average ground truth position (x, y, z) from all training datasets\n",
      "estimates the drone ground truth position, which is (0.734, -9.739, 33.353).\n",
      "2.3 Implementation Details\n",
      "The program fetches LiDAR 360 or Livox Avia data from the nearest timestamp for each sequence,\n",
      "as indicated in the test dataset. Clustering is executed using the DBSCAN algorithm with appro-\n",
      "priate parameters to guarantee strong clustering. Visual inspection is employed for the preliminary\n",
      "separation of points, ensuring an accurate categorization of environment points.\n",
      "The implementation was conducted on a Lenovo IdeaPad Slim 5 Pro (16\") running Windows 11\n",
      "with an AMD Ryzen 7 5800H CPU and 16GB DDR4 RAM. The analysis was carried out in a\n",
      "Jupyter Notebook environment using Python 3.10. For clustering, the DBSCAN algorithm from the\n",
      "Scikit-Learn library was utilized. The DBSCAN algorithm was configured with an epsilon (eps)\n",
      "value of 2 and a minimum number of points (minPts) set to 1.\n",
      "3 Results\n",
      "The algorithm achieved a pose MSE loss of 120.215 and a classification accuracy of 0.322. Table 1\n",
      "presents the evaluation results compared to other teams.\n",
      "Table 1: Evaluation results on the leaderboard\n",
      "Team ID Pose MSE ( â†“) Accuracy ( â†‘)\n",
      "SDUCZS 58198 2.21375 0.8136\n",
      "Gaofen Lab 57978 7.299575 0.3220\n",
      "sysutlt 57843 24.50694 0.3220\n",
      "casetrous 58233 56.880267 0.2542\n",
      "NTU-ICG (ours) 58268 120.215107 0.3220\n",
      "MTC 58180 189.669428 0.2724\n",
      "gzist 56936 417.396317 0.2302\n",
      "4 Conclusions\n",
      "This paper introduces a clustering-based learning method, CL-Det, which employs advanced cluster-\n",
      "ing techniques such as K-Means and DBSCAN for drone detection and position estimation using\n",
      "LiDAR data. The approach guarantees dependable and precise drone position estimation by utilizing\n",
      "multi-sensor data and robust clustering methods. Fallback mechanisms are in place to ensure con-\n",
      "tinuous position estimation even when primary sensor data is absent. Through thorough parameter\n",
      "optimization and comparative assessment, the proposed methodâ€™s effective performance in drone\n",
      "tracking and position estimation is demonstrated.\n",
      "3\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\vikra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Function to validate paragraphs\n",
    "def validate_paragraphs(paragraphs, word_threshold=50):\n",
    "    results = []\n",
    "    for para in paragraphs:\n",
    "        sentences = sent_tokenize(para)  # Tokenize into sentences\n",
    "        if len(sentences) == 0:  # No sentences detected\n",
    "            results.append((para, False))  # Invalid paragraph\n",
    "            continue\n",
    "        avg_words_per_sentence = len(para.split()) / len(sentences)\n",
    "        results.append((para, avg_words_per_sentence <= word_threshold))\n",
    "    return results\n",
    "\n",
    "# Split the extracted text into paragraphs\n",
    "paragraphs = pdf_text.split(\"\\n\\n\")  # Assuming double newlines separate paragraphs\n",
    "validation_results = validate_paragraphs(paragraphs)\n",
    "\n",
    "# Display validation results\n",
    "for para, is_valid in validation_results:\n",
    "    print(f\"VALID: {is_valid}\\nParagraph: {para}...\\n\")  # Print first 200 chars for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title and Abstract Misalignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title and Abstract are aligned (Similarity Score: 0.66)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load pre-trained model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Check title-abstract alignment\n",
    "def check_title_abstract_alignment(title, abstract, threshold=0.5):\n",
    "    title_embedding = model.encode(title, convert_to_tensor=True)\n",
    "    abstract_embedding = model.encode(abstract, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(title_embedding, abstract_embedding)\n",
    "    return similarity.item() >= threshold, similarity.item()\n",
    "\n",
    "# Perform the check\n",
    "is_aligned, similarity_score = check_title_abstract_alignment(title, abstract)\n",
    "print(f\"Title and Abstract are {'aligned' if is_aligned else 'misaligned'} (Similarity Score: {similarity_score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstracts having Multiple Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "paragraphs = abstract.strip().split(\"\\n\\n\")\n",
    "multiple_paras = len(paragraphs) > 1\n",
    "print(multiple_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
