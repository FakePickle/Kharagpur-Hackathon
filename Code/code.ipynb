{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (0.11.5)\n",
      "Requirement already satisfied: sentence_transformers in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: click in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from pdfplumber) (11.1.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (44.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from sentence_transformers) (4.47.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from sentence_transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from sentence_transformers) (1.15.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from sentence_transformers) (0.27.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.2.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.12.14)\n",
      "Requirement already satisfied: pycparser in c:\\users\\vikra\\onedrive\\desktop\\kharagpur-hackathon\\.venv\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk pdfplumber sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Text from the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalization in ReLU Networks via Restricted\n",
      "Isometry and Norm Concentration\n",
      "Abstract\n",
      "Regression tasks, while aiming to model relationships across the entire input space,\n",
      "are often constrained by limited training data. Nevertheless, if the hypothesis func-\n",
      "tions can be represented effectively by the data, there is potential for identifying a\n",
      "model that generalizes well. This paper introduces the Neural Restricted Isometry\n",
      "Property (NeuRIPs), which acts as a uniform concentration event that ensures all\n",
      "shallow ReLU networks are sketched with comparable quality. To determine the\n",
      "sample complexity necessary to achieve NeuRIPs, we bound the covering numbers\n",
      "of the networks using the Sub-Gaussian metric and apply chaining techniques. As-\n",
      "suming the NeuRIPs event, we then provide bounds on the expected risk, applicable\n",
      "to networks within any sublevel set of the empirical risk. Our results show that all\n",
      "networks with sufficiently small empirical risk achieve uniform generalization.\n",
      "1 Introduction\n",
      "A fundamental requirement of any scientific model is a clear evaluation of its limitations. In recent\n",
      "years, supervised machine learning has seen the development of tools for automated model discovery\n",
      "from training data. However, these methods often lack a robust theoretical framework to estimate\n",
      "model limitations. Statistical learning theory quantifies the limitation of a trained model by the\n",
      "generalization error. This theory uses concepts such as the VC-dimension and Rademacher complexity\n",
      "to analyze generalization error bounds for classification problems. While these traditional complexity\n",
      "notions have been successful in classification problems, they do not apply to generic regression\n",
      "problems with unbounded risk functions, which are the focus of this study. Moreover, traditional\n",
      "tools in statistical learning theory have not been able to provide a fully satisfying generalization\n",
      "theory for neural networks.\n",
      "Understanding the risk surface during neural network training is crucial for establishing a strong\n",
      "theoretical foundation for neural network-based machine learning, particularly for understanding\n",
      "generalization. Recent studies on neural networks suggest intriguing properties of the risk surface.\n",
      "In large networks, local minima of the risk form a small bond at the global minimum. Surprisingly,\n",
      "global minima exist in each connected component of the risk’s sublevel set and are path-connected.\n",
      "In this work, we contribute to a generalization theory for shallow ReLU networks, by giving uniform\n",
      "generalization error bounds within the empirical risk’s sublevel set. We use methods from the analysis\n",
      "of convex linear regression, where generalization bounds for empirical risk minimizers are derived\n",
      "from recent advancements in stochastic processes’ chaining theory. Empirical risk minimization\n",
      "for non-convex hypothesis functions cannot generally be solved efficiently. However, under certain\n",
      "assumptions, it is still possible to derive generalization error bounds, as we demonstrate in this paper\n",
      "for shallow ReLU networks. Existing works have applied methods from compressed sensing to\n",
      "bound generalization errors for arbitrary hypothesis functions. However, they do not capture the\n",
      "risk’s stochastic nature through the more advanced chaining theory.\n",
      "This paper is organized as follows. We begin in Section II by outlining our assumptions about the\n",
      "parameters of shallow ReLU networks and the data distribution to be interpolated. The expected and\n",
      "empirical risk are introduced in Section III, where we define the Neural Restricted Isometry Property\n",
      ".\n",
      "(NeuRIPs) as a uniform norm concentration event. We present a bound on the sample complexity for\n",
      "achieving NeuRIPs in Theorem 1, which depends on both the network architecture and parameter\n",
      "assumptions. We provide upper bounds on the generalization error that are uniformly applicable\n",
      "across the sublevel sets of the empirical risk in Section IV . We prove this property in a network\n",
      "recovery setting in Theorem 2, and also an agnostic learning setting in Theorem 3. These results\n",
      "ensure a small generalization error, when any optimization algorithm finds a network with a small\n",
      "empirical risk. We develop the key proof techniques for deriving the sample complexity of achieving\n",
      "NeuRIPs in Section V , by using the chaining theory of stochastic processes. The derived results are\n",
      "summarized in Section VI, where we also explore potential future research directions.\n",
      "2 Notation and Assumptions\n",
      "In this section, we will define the key notations and assumptions for the neural networks examined\n",
      "in this study. A Rectified Linear Unit (ReLU) function ϕ:R→Ris given by ϕ(x) := max( x,0).\n",
      "Given a weight vector w∈Rd, a bias b∈R, and a sign κ∈ {± 1}, a ReLU neuron is a function\n",
      "ϕ(w, b, κ ) :Rd→Rdefined as\n",
      "ϕ(w, b, κ )(x) =κϕ(wTx+b).\n",
      "Shallow neural networks are constructed as weighted sums of neurons. Typically they are represented\n",
      "by a graph with nneurons in a single hidden layer. When using the ReLU activation function, we can\n",
      "apply a symmetry procedure to represent these as sums:\n",
      "¯ϕ¯p(x) =nX\n",
      "i=0ϕpi(x),\n",
      "where ¯pis the tuple (p1, . . . , p n).\n",
      "Assumption 1. The parameters ¯p, which index shallow ReLU networks, are drawn from a set\n",
      "¯P⊆(Rd×R× {± 1})n.\n",
      "For¯P, we assume there exist constants cw≥0andcb∈[1,3], such that for all parameter tuples\n",
      "¯p={(w1, b1, κ1), . . . , (wn, bn, κn)} ∈¯P, we have\n",
      "∥wi∥ ≤cwand|bi| ≤cb.\n",
      "We denote the set of shallow networks indexed by a parameter set ¯Pby\n",
      "Φ¯P:={ϕ¯p: ¯p∈¯P}.\n",
      "We now equip the input space Rdof the networks with a probability distribution. This distribution\n",
      "reflects the sampling process and makes each neural network a random variable. Additionally, a\n",
      "random label ytakes its values in the output space R, for which we assume the following.\n",
      "Assumption 2. The random sample x∈Rdand label y∈Rfollow a joint distribution µsuch that\n",
      "the marginal distribution µxof sample x is standard Gaussian with density\n",
      "1\n",
      "(2π)d/2exp\u0012\n",
      "−∥x∥2\n",
      "2\u0013\n",
      ".\n",
      "As available data, we assume independent copies {(xj, yj)}m\n",
      "j=1of the random pair (x, y), each\n",
      "distributed by µ.\n",
      "3 Concentration of the Empirical Norm\n",
      "Supervised learning algorithms interpolate labels yfor samples x, both distributed jointly by µon\n",
      "X × Y . This task is often solved under limited data accessibility. The training data, respecting\n",
      "Assumption 2, consists of mindependent copies of the random pair (x, y). During training, the\n",
      "interpolation quality of a hypothesis function f:X → Y can only be assessed at the given random\n",
      "samples {xj}m\n",
      "j=1. Any algorithm therefore accesses each function fthrough its sketch samples\n",
      "S[f] = (f(x1), . . . , f (xm)),\n",
      "2\n",
      "where Sis the sample operator. After training, the quality of a resulting model is often measured by\n",
      "its generalization to new data not used during training. With Rd×Ras the input and output space,\n",
      "we quantify a function f’s generalization error with its expected risk:\n",
      "Eµ[f] :=Eµ|y−f(x)|2.\n",
      "The functional || · || µ, also gives the norm of the space L2(Rd, µx), which consists of functions\n",
      "f:Rd→Rwith\n",
      "∥f∥2\n",
      "µ:=Eµx[|f(x)|2].\n",
      "If the label ydepends deterministically on the associated sample x, we can treat yas an element of\n",
      "L2(Rd, µx), and the expected risk of any function fis the function’s distance to y. By sketching any\n",
      "hypothesis function fwith the sample operator S, we perform a Monte-Carlo approximation of the\n",
      "expected risk, which is termed the empirical risk:\n",
      "∥f∥2\n",
      "m:=1\n",
      "mmX\n",
      "2√m(y1, . . . , y m)T−S[f]\n",
      "2.\n",
      "The random functional || · || malso defines a seminorm on L2(Rd, µx), referred to as the empirical\n",
      "norm. Under mild assumptions, || · || mfails to be a norm.\n",
      "In order to obtain a well generalizing model, the goal is to identify a function fwith a low expected\n",
      "risk. However, with limited data, we are restricted to optimizing the empirical risk. Our strategy for\n",
      "deriving generalization guarantees is based on the stochastic relation between both risks. If {xj}m\n",
      "j=1\n",
      "are independently distributed by µx, the law of large numbers implies that for any f∈L2(Rd, µx)\n",
      "the convergence\n",
      "lim\n",
      "m→∞∥f∥m=∥f∥µ.\n",
      "While this establishes the asymptotic convergence of the empirical norm to the function norm for a\n",
      "single function f, we have to consider two issues to formulate our concept of norm concentration:\n",
      "First, we need non-asymptotic results, that is bounds on the distance |∥f∥m− ∥f∥µ|for a fixed\n",
      "number of samples m. Second, the bounds on the distance need to be uniformly valid for all functions\n",
      "fin a given set.\n",
      "Sample operators which have uniform concentration properties have been studied as restricted\n",
      "isometries in the area of compressed sensing. For shallow ReLU networks of the form (1), we define\n",
      "the restricted isometry property of the sampling operator Sas follows.\n",
      "Definition 1. Lets∈(0,1)be a constant and ¯Pbe a parameter set. We say that the Neural Restricted\n",
      "Isometry Property (NeuRIPs( ¯P)) is satisfied if, for all ¯p∈¯Pit holds that\n",
      "(1−s)∥ϕ¯p∥µ≤ ∥ϕ¯p∥m≤(1 +s)∥ϕ¯p∥µ.\n",
      "In the following Theorem, we provide a bound on the number mof samples, which is sufficient for\n",
      "the operator Sto satisfy NeuRIPs( ¯P).\n",
      "Theorem 1. There exist universal constants C1,C2∈Rsuch that the following holds: For\n",
      "any sample operator S, constructed from random samples {xj}, respecting Assumption 2, let\n",
      "¯P⊂(Rd×R× {± 1})nbe any parameter set satisfying Assumption 1 and ||ϕ¯p||µ>1for all\n",
      "¯p∈¯P. Then, for any u > 2ands∈(0,1), NeuRIPs( ¯P) is satisfied with probability at least\n",
      "1−17 exp( −u/4)provided that\n",
      "m≥n3c2\n",
      "w\n",
      "(1−s)2max\u0012\n",
      "C1(8cb+d+ ln(2))\n",
      "u, C2n2c2\n",
      "w\n",
      "(u/s)2\u0013\n",
      ".\n",
      "One should notice that, in Theorem 1, there is a tradeoff between the parameter s, which limits the\n",
      "deviation |∥ · ∥ m− ∥ · ∥ µ|, and the confidence parameter u. The lower bound on the corresponding\n",
      "sample size mis split into two scaling regimes when understanding the quotient uof|∥·∥m−∥·∥ µ|/s\n",
      "as a precision parameter. While in the regime of low deviations and high probabilities the sample size\n",
      "mmust scale quadratically with u/s, in the regime of less precise statements one observes a linear\n",
      "scaling.\n",
      "3\n",
      "4 Uniform Generalization of Sublevel Sets of the Empirical Risk\n",
      "When the NeuRIPs event occurs, the function norm || · || µ, which is related to the expected risk, is\n",
      "close to || · || m, which corresponds to the empirical risk. Motivated by this property, we aim to find\n",
      "a shallow ReLU network ϕ¯pwith small expected risk by solving the empirical risk minimization\n",
      "problem:\n",
      "min\n",
      "¯p∈¯P∥ϕ¯p−y∥2\n",
      "m.\n",
      "Since the set Φ¯Pof shallow ReLU networks is non-convex, this minimization cannot be solved\n",
      "with efficient convex optimizers. Therefore, instead of analyzing only the solution ϕ∗\n",
      "¯pof the opti-\n",
      "mization problem, we introduce a tolerance ϵ >0for the empirical risk and provide bounds on the\n",
      "generalization error, which hold uniformly on the sublevel set\n",
      "¯Qy,ϵ:\n",
      "¯p∈¯P:∥ϕ¯p−y∥2\n",
      "m≤ϵ\t\n",
      ".\n",
      "Before considering generic regression problems, we will initially assume the label yto be a neural\n",
      "network itself, parameterized by a tuple p∗within the hypothesis set P. For all (x, y)in the support of\n",
      "µ, we have y=ϕp∗(x)and the expected risk’s minimum on Pis zero. Using the sufficient condition\n",
      "for NeuRIPs from Theorem 1, we can provide generalization bounds for ϕ¯p∈¯Qy,ϵfor any ϵ >0.\n",
      "Theorem 2. Let¯Pbe a parameter set that satisfies Assumption 1 and let u≥2andt≥ϵ >0be\n",
      "constants. Furthermore, let the number mof samples satisfy\n",
      "m≥8n3c2\n",
      "w(8cb+d+ ln(2)) max\u0012\n",
      "C1u\n",
      "(t−ϵ)2, C2n2c2\n",
      "wu\n",
      "(t−ϵ)2\u0013\n",
      ",\n",
      "where C1andC2are universal constants. Let {(xj, yj)}m\n",
      "j=1be a dataset respecting Assumption 2\n",
      "and let there exist a ¯p∗∈¯Psuch that yj=ϕ¯p∗(xj)holds for all j∈[m]. Then, with probability at\n",
      "least1−17 exp( −u/4), we have for all ¯q∈¯Qy,ϵthat\n",
      "∥ϕ¯q−ϕ¯p∗∥2\n",
      "µ≤t.\n",
      "Proof. We notice that ¯Qy,ϵis a set of shallow neural networks with 2nneurons. We normalize such\n",
      "networks with a function norm greater than tand parameterize them by\n",
      "¯Rt:={ϕ¯p−ϕ¯p∗: ¯p∈¯P,∥ϕ¯p−ϕ¯p∗∥µ> t}.\n",
      "We assume that NeuRIPs( ¯Rt) holds for s= (t−ϵ)2/t2. In this case, for all ¯q∈¯Qy,ϵ, we have that\n",
      "∥ϕ¯q−ϕ¯p∗∥m≥tand thus ¯q /∈¯Qϕ¯p∗,ϵ, which implies that ∥ϕ¯q−ϕ¯p∗∥µ≤t.\n",
      "We also note that ¯Rtsatisfies Assumption 1 with a rescaled constant cw/tand normalization-invariant\n",
      "cb, if¯Psatisfies it for cwandcb. Theorem 1 gives a lower bound on the sample complexity for\n",
      "NeuRIPs( ¯Rt), completing the proof.\n",
      "At any network where an optimization method terminates, the concentration of the empirical risk\n",
      "at the expected risk can be achieved with less data than needed to achieve an analogous NeuRIPs\n",
      "event. However, in the chosen stochastic setting, we cannot assume that the termination of an\n",
      "optimization and the norm concentration at that network are independent events. We overcome this\n",
      "by not specifying the outcome of an optimization method and instead stating uniform bounds on\n",
      "the norm concentration. The only assumption on an algorithm is therefore the identification of a\n",
      "network that permits an upper bound ϵon its empirical risk. The event NeuRIPs( ¯Rt) then restricts the\n",
      "expected risk to be below the corresponding level t.\n",
      "We now discuss the empirical risk surface for generic distributions µthat satisfy Assumption 2, where\n",
      "ydoes not necessarily have to be a neural network.\n",
      "Theorem 3. There exist constants C0,C1,C2,C3,C4, and C5such that the following holds: Let ¯P\n",
      "satisfy Assumption 1 for some constants cw,cb, and let ¯p∗∈¯Pbe such that for some c¯p∗≥0we\n",
      "have\n",
      "Eµ\u0014\n",
      "exp\u0012(y−ϕ¯p∗(x))2\n",
      "c2\n",
      "¯p∗\u0013\u0015\n",
      "≤2.\n",
      "We assume, for any s∈(0,1)and confidence parameter u >0, that the number of samples mis\n",
      "large enough such that\n",
      "m≥8\n",
      "(1−s)2max\u0012\n",
      "C1\u0012n3c2\n",
      "w(8cb+d+ ln(2))\n",
      "u\u0013\n",
      ", C2n2c2\n",
      "w\u0010u\n",
      "s\u0011\u0013\n",
      ".\n",
      "4\n",
      "We further select confidence parameters v1, v2> C 0, and define for some ω≥0the parameter\n",
      "η:= 2(1 −s)∥ϕ¯p∗−y∥µ+C3v1v2c¯p∗1\n",
      "(1−s)1/4+ω√\n",
      "1−s.\n",
      "If we set ϵ=∥ϕ¯p∗−y∥2\n",
      "m+ω2as the tolerance for the empirical risk, then the probability that all\n",
      "¯q∈¯Qy,ϵsatisfy\n",
      "∥ϕ¯q−y∥µ≤η\n",
      "is at least\n",
      "1−17 exp\u0010\n",
      "−u\n",
      "4\u0011\n",
      "−C5v2exp\u0012\n",
      "−C4mv2\n",
      "2\n",
      "2\u0013\n",
      ".\n",
      "Proof sketch. (Complete proof in Appendix E) We first define and decompose the excess risk by\n",
      "E(¯q,¯p∗) :=∥ϕ¯q−y∥2\n",
      "µ− ∥ϕ¯p∗−y∥2\n",
      "µ=∥ϕ¯q−ϕ¯p∗∥2\n",
      "µ−2\n",
      "mmX\n",
      "j=1(ϕ¯p∗(xj)−yj)(ϕ¯q(xj)−ϕ¯p∗(xj)).\n",
      "It suffices to show, that within the stated confidence level we have ∥ϕ¯q−y∥µ> η. This implies the\n",
      "claim since ∥ϕ¯q−y∥m≤ϵimplies ∥ϕ¯q−y∥µ≤η. We have E[E(¯q,¯p∗)]>0. It now only remains\n",
      "to strengthen the condition on η >3∥ϕ¯p∗−y∥µto achieve E(¯q,¯p∗)> ω2. We apply Theorem 1\n",
      "to derive a bound on the fluctuation of the first term. The concentration rate of the second term is\n",
      "derived similar to Theorem 1 by using chaining techniques. Finally in Appendix E, Theorem 12 gives\n",
      "a general bound to achieve\n",
      "E(¯q,¯p∗)> ω2\n",
      "uniformly for all ¯qwith∥ϕ¯q−ϕ¯p∗∥µ> η. Theorem 3 then follows as a simplification.\n",
      "It is important to notice that, in Theorem 3, as the data size mapproaches infinity, one can select\n",
      "an asymptotically small deviation constant s. In this limit, the bound ηon the generalization error\n",
      "converges to 3∥ϕ¯p∗−y∥µ+ω. This reflects a lower limit of the generalization bound, which is the\n",
      "sum of the theoretically achievable minimum of the expected risk and the additional tolerance ω.\n",
      "The latter is an upper bound on the empirical risk, which real-world optimization algorithms can be\n",
      "expected to achieve.\n",
      "5 Size Control of Stochastic Processes on Shallow Networks\n",
      "In this section, we introduce the key techniques for deriving concentration statements for the em-\n",
      "pirical norm, uniformly valid for sets of shallow ReLU networks. We begin by rewriting the event\n",
      "NeuRIPs( ¯P) by treating µas a stochastic process, indexed by the parameter set ¯P. The event\n",
      "NeuRIPs( ¯P) holds if and only if we have\n",
      "sup\n",
      "¯p∈¯P|∥ϕ¯p∥m− ∥ϕ¯p∥µ| ≤ssup\n",
      "¯p∈¯P∥ϕ¯p∥µ.\n",
      "The supremum of stochastic processes has been studied in terms of their size. To determine the size\n",
      "of a process, it is essential to determine the correlation between its variables. To this end, we define\n",
      "the Sub-Gaussian metric for any parameter tuples ¯p,¯q∈¯Pas\n",
      "dψ2(ϕ¯p, ϕ¯q) := inf(\n",
      "Cψ2≥0 :E\"\n",
      "exp \n",
      "|ϕ¯p(x)−ϕ¯q(x)|2\n",
      "C2\n",
      "ψ2!#\n",
      "≤2)\n",
      ".\n",
      "A small Sub-Gaussian metric between random variables indicates that their values are likely to be\n",
      "close. To capture the Sub-Gaussian structure of a process, we introduce ϵ-nets in the Sub-Gaussian\n",
      "metric. For a given ϵ >0, these are subsets ¯Q⊆¯Psuch that for every ¯p∈¯P, there is a ¯q∈¯Q\n",
      "satisfying\n",
      "dψ2(ϕ¯p, ϕ¯q)≤ϵ.\n",
      "The smallest cardinality of such an ϵ-net ¯Qis known as the Sub-Gaussian covering number\n",
      "N(Φ¯P, dψ2, ϵ). The next Lemma offers a bound for such covering numbers specific to shallow\n",
      "ReLU networks.\n",
      "5\n",
      "Lemma 1. Let¯Pbe a parameter set satisfying Assumption 1. Then there exists a set ˆPwith ¯P⊆ˆP\n",
      "such that\n",
      "N(ΦˆP, dψ2, ϵ)≤2n·\u001216ncbcw\n",
      "ϵ+ 1\u0013n\n",
      "·\u001232ncbcw\n",
      "ϵ+ 1\u0013n\n",
      "·\u00121\n",
      "ϵsin\u00121\n",
      "16ncw\u0013\n",
      "+ 1\u0013d\n",
      ".\n",
      "The proof of this Lemma is based on the theory of stochastic processes and can be seen in Theorem 8\n",
      "of Appendix C.\n",
      "To obtain bounds of the form (6) on the size of a process, we use the generic chaining method. This\n",
      "method offers bounds in terms of the Talagrand-functional of the process in the Sub-Gaussian metric.\n",
      "We define it as follows. A sequence T= (Tk)k∈N0in a set Tis admissible if T0= 1andTk≤2(2k).\n",
      "The Talagrand-functional of the metric space is then defined as\n",
      "γ2(T, d) := inf\n",
      "(Tk)sup\n",
      "t∈T∞X\n",
      "k=02kd(t, Tk),\n",
      "where the infimum is taken across all admissible sequences.\n",
      "With the bounds on the Sub-Gaussian covering number from Lemma 1, we provide a bound on the\n",
      "Talagrand-functional for shallow ReLU networks in the following Lemma. This bound is expected to\n",
      "be of independent interest.\n",
      "Lemma 2. Let¯Psatisfy Assumption 1. Then we have\n",
      "γ2(Φ¯P, dψ2)≤r\n",
      "2\n",
      "π\u00128n3/2cw(8cb+d+ 1)\n",
      "ln(2)p\n",
      "2 ln(2)\u0013\n",
      ".\n",
      "The key ideas to show this bound are similar to the ones used to prove Theorem 9 in Appendix C.\n",
      "To provide bounds for the empirical process, we use the following Lemma, which we prove in\n",
      "Appendix D.\n",
      "Lemma 3. LetΦbe a set of real functions, indexed by a parameter set ¯Pand define\n",
      "N(Φ) :=Z∞\n",
      "0q\n",
      "lnN(Φ, dψ2, ϵ)dϵ and ∆(Φ) := sup\n",
      "ϕ∈Φ∥ϕ∥ψ2.\n",
      "Then, for any u≥2, we have with probability at least 1−17 exp( −u/4)that\n",
      "sup\n",
      "ϕ∈Φ|∥ϕ∥m− ∥ϕ∥µ| ≤u√m\u0014\n",
      "N(Φ) +10\n",
      "3∆(Φ)\u0015\n",
      ".\n",
      "The bounds on the sample complexity for achieving the NeuRIPs event, from Theorem 1, are proven\n",
      "by applying these Lemmata.\n",
      "Proof of Theorem 1. Since we assume ||ϕ¯p||µ>1for all ¯p∈¯P, we have\n",
      "sup\n",
      "¯p∈¯P|∥ϕ¯p∥m− ∥ϕ¯p∥µ| ≤sup\n",
      "¯p∈¯P|∥ϕ¯p∥m− ∥ϕ¯p∥µ|/∥ϕ¯p∥µ.\n",
      "Applying Lemma 3, and further applying the bounds on the covering numbers and the Talagrand-\n",
      "functional for shallow ReLU networks, the NeuRIPs( ¯P) event holds in case of s >3. The sample\n",
      "complexities that are provided in Theorem 1 follow from a refinement of this condition.\n",
      "6 Uniform Generalization of Sublevel Sets of the Empirical Risk\n",
      "In case of the NeuRIPs event, the function norm || · || µcorresponding to the expected risk is close\n",
      "to|| · || m, which corresponds to the empirical risk. With the previous results, we can now derive\n",
      "uniform generalization error bounds in the sublevel set of the empirical risk.\n",
      "We use similar techniques and we define the following sets.\n",
      "∥f∥p= sup\n",
      "1≤q≤p∥f∥q\n",
      "Λk0,u= inf\n",
      "(Tk)sup\n",
      "f∈F∞X\n",
      "k02k∥f−Tk(f)∥u2k\n",
      "6\n",
      "and we need the following lemma:\n",
      "Lemma 9. For any set Fof functions and u≥1, we have\n",
      "Λ0,u(F)≤2√e(γ2(F, dψ2) + ∆( F)).\n",
      "Theorem 10. Let P be a parameter set satisfying Assumption 1. Then, for any u≥1, we have with\n",
      "probability at least 1−17 exp( −u/4)that\n",
      "sup\n",
      "¯p∈P∥ϕ¯p∥m− ∥ϕ¯p∥µ≤u√m\u0010\n",
      "16n3/2cw(8cb+d+ 1) + 2 ncw\u0011\n",
      ".\n",
      "Proof. To this end we have to bound the Talagrand functional, where we can use Dudley’s inequality\n",
      "(Lemma 6). To finish the proof, we apply the bounds on the covering numbers provided by Theorem\n",
      "6.\n",
      "Theorem 11. Let¯P⊆(Rd×R× ±1)nsatisfy Assumption 1. Then there exist universal constants\n",
      "C1,C2such that\n",
      "sup\n",
      "¯p∈P∥ϕ¯p∥m− ∥ϕ¯p∥µ≤r\n",
      "2\n",
      "π\u00128n3/2cw(8cb+d+ 1)\n",
      "ln(2)p\n",
      "2 ln(2)\u0013\n",
      ".\n",
      "7 Conclusion\n",
      "In this study, we investigated the empirical risk surface of shallow ReLU networks in terms of uniform\n",
      "concentration events for the empirical norm. We defined the Neural Restricted Isometry Property\n",
      "(NeuRIPs) and determined the sample complexity required to achieve NeuRIPs, which depends on\n",
      "realistic parameter bounds and the network architecture. We applied our findings to derive upper\n",
      "bounds on the expected risk, which are valid uniformly across sublevel sets of the empirical risk.\n",
      "If a network optimization algorithm can identify a network with a small empirical risk, our results\n",
      "guarantee that this network will generalize well. By deriving uniform concentration statements, we\n",
      "have resolved the problem of independence between the termination of an optimization algorithm at\n",
      "a certain network and the empirical risk concentration at that network. Future studies may focus on\n",
      "performing uniform empirical norm concentration on the critical points of the empirical risk, which\n",
      "could lead to even tighter bounds for the sample complexity.\n",
      "We also plan to apply our methods to input distributions more general than the Gaussian distribution.\n",
      "If generic Gaussian distributions can be handled, one could then derive bounds for the Sub-Gaussian\n",
      "covering number for deep ReLU networks by induction across layers. We also expect that our\n",
      "results on the covering numbers could be extended to more generic Lipschitz continuous activation\n",
      "functions other than ReLU. This proposition is based on the concentration of measure phenomenon,\n",
      "which provides bounds on the Sub-Gaussian norm of functions on normal concentrating input spaces.\n",
      "Because these bounds scale with the Lipschitz constant of the function, they can be used to find ϵ-nets\n",
      "for neurons that have identical activation patterns.\n",
      "Broader Impact\n",
      "Supervised machine learning now affects both personal and public lives significantly. Generalization is\n",
      "critical to the reliability and safety of empirically trained models. Our analysis aims to achieve a deeper\n",
      "understanding of the relationships between generalization, architectural design, and available data.\n",
      "We have discussed the concepts and demonstrated the effectiveness of using uniform concentration\n",
      "events for generalization guarantees of common supervised machine learning algorithms.\n",
      "7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file using PyPDF2.\n",
    "    :param file_path: Path to the PDF file.\n",
    "    :return: Extracted text as a single string.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"  # Extract text page by page\n",
    "    return text\n",
    "\n",
    "# file_path = \"..\\Dataset\\Papers\\P001.pdf\"\n",
    "file_path = \"../Dataset/Reference/Publishable/NeurIPS/R013.pdf\"\n",
    "pdf_text = extract_text_from_pdf(file_path)\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title and Abstract Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:\n",
      " Generalization in ReLU Networks via Restricted\n",
      "Isometry and Norm Concentration\n",
      "\n",
      "Abstract:\n",
      " Regression tasks, while aiming to model relationships across the entire input space,\n",
      "are often constrained by limited training data. Nevertheless, if the hypothesis func-\n",
      "tions can be represented effectively by the data, there is potential for identifying a\n",
      "model that generalizes well. This paper introduces the Neural Restricted Isometry\n",
      "Property (NeuRIPs), which acts as a uniform concentration event that ensures all\n",
      "shallow ReLU networks are sketched with comparable quality. To determine the\n",
      "sample complexity necessary to achieve NeuRIPs, we bound the covering numbers\n",
      "of the networks using the Sub-Gaussian metric and apply chaining techniques. As-\n",
      "suming the NeuRIPs event, we then provide bounds on the expected risk, applicable\n",
      "to networks within any sublevel set of the empirical risk. Our results show that all\n",
      "networks with sufficiently small empirical risk achieve uniform generalization.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Function to dynamically extract title and abstract\n",
    "def extract_title_and_abstract(pdf_text):\n",
    "    \"\"\"\n",
    "    Extract the title and abstract dynamically from the PDF text.\n",
    "    Title: From the start to \"Abstract\".\n",
    "    Abstract: From \"Abstract\" to \"Introduction\".\n",
    "    \"\"\"\n",
    "    title, abstract = \"\", \"\"\n",
    "    try:\n",
    "        # Extract Title\n",
    "        title_end_idx = pdf_text.index(\"Abstract\")\n",
    "        title = pdf_text[:title_end_idx].strip()\n",
    "\n",
    "        # Extract Abstract\n",
    "        abstract_start_idx = pdf_text.index(\"Abstract\") + len(\"Abstract\")\n",
    "        abstract_end_idx = pdf_text.index(\"Introduction\")\n",
    "        abstract = pdf_text[abstract_start_idx:abstract_end_idx].strip()\n",
    "    except ValueError as e:\n",
    "        print(f\"Error extracting title and abstract: {e}\")\n",
    "    \n",
    "    return title, abstract\n",
    "\n",
    "title, abstract = extract_title_and_abstract(pdf_text)\n",
    "\n",
    "print(\"Title:\\n\", title)\n",
    "print(\"\\nAbstract:\\n\", abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('networks', np.float64(0.3730019232961255)),\n",
      " ('neurips', np.float64(0.27975144247209416)),\n",
      " ('risk', np.float64(0.27975144247209416)),\n",
      " ('achieve', np.float64(0.18650096164806276)),\n",
      " ('data', np.float64(0.18650096164806276)),\n",
      " ('empirical', np.float64(0.18650096164806276)),\n",
      " ('event', np.float64(0.18650096164806276)),\n",
      " ('model', np.float64(0.18650096164806276)),\n",
      " ('uniform', np.float64(0.18650096164806276)),\n",
      " ('acts', np.float64(0.09325048082403138)),\n",
      " ('aiming', np.float64(0.09325048082403138)),\n",
      " ('applicable', np.float64(0.09325048082403138)),\n",
      " ('apply', np.float64(0.09325048082403138)),\n",
      " ('bound', np.float64(0.09325048082403138)),\n",
      " ('bounds', np.float64(0.09325048082403138)),\n",
      " ('chaining', np.float64(0.09325048082403138)),\n",
      " ('comparable', np.float64(0.09325048082403138)),\n",
      " ('complexity', np.float64(0.09325048082403138)),\n",
      " ('concentration', np.float64(0.09325048082403138)),\n",
      " ('constrained', np.float64(0.09325048082403138)),\n",
      " ('covering', np.float64(0.09325048082403138)),\n",
      " ('determine', np.float64(0.09325048082403138)),\n",
      " ('effectively', np.float64(0.09325048082403138)),\n",
      " ('ensures', np.float64(0.09325048082403138)),\n",
      " ('entire', np.float64(0.09325048082403138)),\n",
      " ('expected', np.float64(0.09325048082403138)),\n",
      " ('func', np.float64(0.09325048082403138)),\n",
      " ('gaussian', np.float64(0.09325048082403138)),\n",
      " ('generalization', np.float64(0.09325048082403138)),\n",
      " ('generalizes', np.float64(0.09325048082403138)),\n",
      " ('hypothesis', np.float64(0.09325048082403138)),\n",
      " ('identifying', np.float64(0.09325048082403138)),\n",
      " ('input', np.float64(0.09325048082403138)),\n",
      " ('introduces', np.float64(0.09325048082403138)),\n",
      " ('isometry', np.float64(0.09325048082403138)),\n",
      " ('limited', np.float64(0.09325048082403138)),\n",
      " ('metric', np.float64(0.09325048082403138)),\n",
      " ('necessary', np.float64(0.09325048082403138)),\n",
      " ('neural', np.float64(0.09325048082403138)),\n",
      " ('numbers', np.float64(0.09325048082403138)),\n",
      " ('paper', np.float64(0.09325048082403138)),\n",
      " ('potential', np.float64(0.09325048082403138)),\n",
      " ('property', np.float64(0.09325048082403138)),\n",
      " ('provide', np.float64(0.09325048082403138)),\n",
      " ('quality', np.float64(0.09325048082403138)),\n",
      " ('regression', np.float64(0.09325048082403138)),\n",
      " ('relationships', np.float64(0.09325048082403138)),\n",
      " ('relu', np.float64(0.09325048082403138)),\n",
      " ('represented', np.float64(0.09325048082403138)),\n",
      " ('restricted', np.float64(0.09325048082403138)),\n",
      " ('results', np.float64(0.09325048082403138)),\n",
      " ('sample', np.float64(0.09325048082403138)),\n",
      " ('set', np.float64(0.09325048082403138)),\n",
      " ('shallow', np.float64(0.09325048082403138)),\n",
      " ('sketched', np.float64(0.09325048082403138)),\n",
      " ('small', np.float64(0.09325048082403138)),\n",
      " ('space', np.float64(0.09325048082403138)),\n",
      " ('sub', np.float64(0.09325048082403138)),\n",
      " ('sublevel', np.float64(0.09325048082403138)),\n",
      " ('sufficiently', np.float64(0.09325048082403138)),\n",
      " ('suming', np.float64(0.09325048082403138)),\n",
      " ('tasks', np.float64(0.09325048082403138)),\n",
      " ('techniques', np.float64(0.09325048082403138)),\n",
      " ('tions', np.float64(0.09325048082403138)),\n",
      " ('training', np.float64(0.09325048082403138)),\n",
      " ('using', np.float64(0.09325048082403138))]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform([abstract])\n",
    "\n",
    "# Extract keywords and their scores\n",
    "scores = zip(vectorizer.get_feature_names_out(), X.toarray()[0])\n",
    "sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Top keywords\n",
    "keywords = sorted_scores  # Adjust the number as needed\n",
    "\n",
    "pprint(sorted_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Paragraphs with No Full Stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID: True\n",
      "Paragraph: Generalization in ReLU Networks via Restricted\n",
      "Isometry and Norm Concentration\n",
      "Abstract\n",
      "Regression tasks, while aiming to model relationships across the entire input space,\n",
      "are often constrained by limited training data. Nevertheless, if the hypothesis func-\n",
      "tions can be represented effectively by the data, there is potential for identifying a\n",
      "model that generalizes well. This paper introduces the Neural Restricted Isometry\n",
      "Property (NeuRIPs), which acts as a uniform concentration event that ensures all\n",
      "shallow ReLU networks are sketched with comparable quality. To determine the\n",
      "sample complexity necessary to achieve NeuRIPs, we bound the covering numbers\n",
      "of the networks using the Sub-Gaussian metric and apply chaining techniques. As-\n",
      "suming the NeuRIPs event, we then provide bounds on the expected risk, applicable\n",
      "to networks within any sublevel set of the empirical risk. Our results show that all\n",
      "networks with sufficiently small empirical risk achieve uniform generalization.\n",
      "1 Introduction\n",
      "A fundamental requirement of any scientific model is a clear evaluation of its limitations. In recent\n",
      "years, supervised machine learning has seen the development of tools for automated model discovery\n",
      "from training data. However, these methods often lack a robust theoretical framework to estimate\n",
      "model limitations. Statistical learning theory quantifies the limitation of a trained model by the\n",
      "generalization error. This theory uses concepts such as the VC-dimension and Rademacher complexity\n",
      "to analyze generalization error bounds for classification problems. While these traditional complexity\n",
      "notions have been successful in classification problems, they do not apply to generic regression\n",
      "problems with unbounded risk functions, which are the focus of this study. Moreover, traditional\n",
      "tools in statistical learning theory have not been able to provide a fully satisfying generalization\n",
      "theory for neural networks.\n",
      "Understanding the risk surface during neural network training is crucial for establishing a strong\n",
      "theoretical foundation for neural network-based machine learning, particularly for understanding\n",
      "generalization. Recent studies on neural networks suggest intriguing properties of the risk surface.\n",
      "In large networks, local minima of the risk form a small bond at the global minimum. Surprisingly,\n",
      "global minima exist in each connected component of the risk’s sublevel set and are path-connected.\n",
      "In this work, we contribute to a generalization theory for shallow ReLU networks, by giving uniform\n",
      "generalization error bounds within the empirical risk’s sublevel set. We use methods from the analysis\n",
      "of convex linear regression, where generalization bounds for empirical risk minimizers are derived\n",
      "from recent advancements in stochastic processes’ chaining theory. Empirical risk minimization\n",
      "for non-convex hypothesis functions cannot generally be solved efficiently. However, under certain\n",
      "assumptions, it is still possible to derive generalization error bounds, as we demonstrate in this paper\n",
      "for shallow ReLU networks. Existing works have applied methods from compressed sensing to\n",
      "bound generalization errors for arbitrary hypothesis functions. However, they do not capture the\n",
      "risk’s stochastic nature through the more advanced chaining theory.\n",
      "This paper is organized as follows. We begin in Section II by outlining our assumptions about the\n",
      "parameters of shallow ReLU networks and the data distribution to be interpolated. The expected and\n",
      "empirical risk are introduced in Section III, where we define the Neural Restricted Isometry Property\n",
      ".\n",
      "(NeuRIPs) as a uniform norm concentration event. We present a bound on the sample complexity for\n",
      "achieving NeuRIPs in Theorem 1, which depends on both the network architecture and parameter\n",
      "assumptions. We provide upper bounds on the generalization error that are uniformly applicable\n",
      "across the sublevel sets of the empirical risk in Section IV . We prove this property in a network\n",
      "recovery setting in Theorem 2, and also an agnostic learning setting in Theorem 3. These results\n",
      "ensure a small generalization error, when any optimization algorithm finds a network with a small\n",
      "empirical risk. We develop the key proof techniques for deriving the sample complexity of achieving\n",
      "NeuRIPs in Section V , by using the chaining theory of stochastic processes. The derived results are\n",
      "summarized in Section VI, where we also explore potential future research directions.\n",
      "2 Notation and Assumptions\n",
      "In this section, we will define the key notations and assumptions for the neural networks examined\n",
      "in this study. A Rectified Linear Unit (ReLU) function ϕ:R→Ris given by ϕ(x) := max( x,0).\n",
      "Given a weight vector w∈Rd, a bias b∈R, and a sign κ∈ {± 1}, a ReLU neuron is a function\n",
      "ϕ(w, b, κ ) :Rd→Rdefined as\n",
      "ϕ(w, b, κ )(x) =κϕ(wTx+b).\n",
      "Shallow neural networks are constructed as weighted sums of neurons. Typically they are represented\n",
      "by a graph with nneurons in a single hidden layer. When using the ReLU activation function, we can\n",
      "apply a symmetry procedure to represent these as sums:\n",
      "¯ϕ¯p(x) =nX\n",
      "i=0ϕpi(x),\n",
      "where ¯pis the tuple (p1, . . . , p n).\n",
      "Assumption 1. The parameters ¯p, which index shallow ReLU networks, are drawn from a set\n",
      "¯P⊆(Rd×R× {± 1})n.\n",
      "For¯P, we assume there exist constants cw≥0andcb∈[1,3], such that for all parameter tuples\n",
      "¯p={(w1, b1, κ1), . . . , (wn, bn, κn)} ∈¯P, we have\n",
      "∥wi∥ ≤cwand|bi| ≤cb.\n",
      "We denote the set of shallow networks indexed by a parameter set ¯Pby\n",
      "Φ¯P:={ϕ¯p: ¯p∈¯P}.\n",
      "We now equip the input space Rdof the networks with a probability distribution. This distribution\n",
      "reflects the sampling process and makes each neural network a random variable. Additionally, a\n",
      "random label ytakes its values in the output space R, for which we assume the following.\n",
      "Assumption 2. The random sample x∈Rdand label y∈Rfollow a joint distribution µsuch that\n",
      "the marginal distribution µxof sample x is standard Gaussian with density\n",
      "1\n",
      "(2π)d/2exp\u0012\n",
      "−∥x∥2\n",
      "2\u0013\n",
      ".\n",
      "As available data, we assume independent copies {(xj, yj)}m\n",
      "j=1of the random pair (x, y), each\n",
      "distributed by µ.\n",
      "3 Concentration of the Empirical Norm\n",
      "Supervised learning algorithms interpolate labels yfor samples x, both distributed jointly by µon\n",
      "X × Y . This task is often solved under limited data accessibility. The training data, respecting\n",
      "Assumption 2, consists of mindependent copies of the random pair (x, y). During training, the\n",
      "interpolation quality of a hypothesis function f:X → Y can only be assessed at the given random\n",
      "samples {xj}m\n",
      "j=1. Any algorithm therefore accesses each function fthrough its sketch samples\n",
      "S[f] = (f(x1), . . . , f (xm)),\n",
      "2\n",
      "where Sis the sample operator. After training, the quality of a resulting model is often measured by\n",
      "its generalization to new data not used during training. With Rd×Ras the input and output space,\n",
      "we quantify a function f’s generalization error with its expected risk:\n",
      "Eµ[f] :=Eµ|y−f(x)|2.\n",
      "The functional || · || µ, also gives the norm of the space L2(Rd, µx), which consists of functions\n",
      "f:Rd→Rwith\n",
      "∥f∥2\n",
      "µ:=Eµx[|f(x)|2].\n",
      "If the label ydepends deterministically on the associated sample x, we can treat yas an element of\n",
      "L2(Rd, µx), and the expected risk of any function fis the function’s distance to y. By sketching any\n",
      "hypothesis function fwith the sample operator S, we perform a Monte-Carlo approximation of the\n",
      "expected risk, which is termed the empirical risk:\n",
      "∥f∥2\n",
      "m:=1\n",
      "mmX\n",
      "2√m(y1, . . . , y m)T−S[f]\n",
      "2.\n",
      "The random functional || · || malso defines a seminorm on L2(Rd, µx), referred to as the empirical\n",
      "norm. Under mild assumptions, || · || mfails to be a norm.\n",
      "In order to obtain a well generalizing model, the goal is to identify a function fwith a low expected\n",
      "risk. However, with limited data, we are restricted to optimizing the empirical risk. Our strategy for\n",
      "deriving generalization guarantees is based on the stochastic relation between both risks. If {xj}m\n",
      "j=1\n",
      "are independently distributed by µx, the law of large numbers implies that for any f∈L2(Rd, µx)\n",
      "the convergence\n",
      "lim\n",
      "m→∞∥f∥m=∥f∥µ.\n",
      "While this establishes the asymptotic convergence of the empirical norm to the function norm for a\n",
      "single function f, we have to consider two issues to formulate our concept of norm concentration:\n",
      "First, we need non-asymptotic results, that is bounds on the distance |∥f∥m− ∥f∥µ|for a fixed\n",
      "number of samples m. Second, the bounds on the distance need to be uniformly valid for all functions\n",
      "fin a given set.\n",
      "Sample operators which have uniform concentration properties have been studied as restricted\n",
      "isometries in the area of compressed sensing. For shallow ReLU networks of the form (1), we define\n",
      "the restricted isometry property of the sampling operator Sas follows.\n",
      "Definition 1. Lets∈(0,1)be a constant and ¯Pbe a parameter set. We say that the Neural Restricted\n",
      "Isometry Property (NeuRIPs( ¯P)) is satisfied if, for all ¯p∈¯Pit holds that\n",
      "(1−s)∥ϕ¯p∥µ≤ ∥ϕ¯p∥m≤(1 +s)∥ϕ¯p∥µ.\n",
      "In the following Theorem, we provide a bound on the number mof samples, which is sufficient for\n",
      "the operator Sto satisfy NeuRIPs( ¯P).\n",
      "Theorem 1. There exist universal constants C1,C2∈Rsuch that the following holds: For\n",
      "any sample operator S, constructed from random samples {xj}, respecting Assumption 2, let\n",
      "¯P⊂(Rd×R× {± 1})nbe any parameter set satisfying Assumption 1 and ||ϕ¯p||µ>1for all\n",
      "¯p∈¯P. Then, for any u > 2ands∈(0,1), NeuRIPs( ¯P) is satisfied with probability at least\n",
      "1−17 exp( −u/4)provided that\n",
      "m≥n3c2\n",
      "w\n",
      "(1−s)2max\u0012\n",
      "C1(8cb+d+ ln(2))\n",
      "u, C2n2c2\n",
      "w\n",
      "(u/s)2\u0013\n",
      ".\n",
      "One should notice that, in Theorem 1, there is a tradeoff between the parameter s, which limits the\n",
      "deviation |∥ · ∥ m− ∥ · ∥ µ|, and the confidence parameter u. The lower bound on the corresponding\n",
      "sample size mis split into two scaling regimes when understanding the quotient uof|∥·∥m−∥·∥ µ|/s\n",
      "as a precision parameter. While in the regime of low deviations and high probabilities the sample size\n",
      "mmust scale quadratically with u/s, in the regime of less precise statements one observes a linear\n",
      "scaling.\n",
      "3\n",
      "4 Uniform Generalization of Sublevel Sets of the Empirical Risk\n",
      "When the NeuRIPs event occurs, the function norm || · || µ, which is related to the expected risk, is\n",
      "close to || · || m, which corresponds to the empirical risk. Motivated by this property, we aim to find\n",
      "a shallow ReLU network ϕ¯pwith small expected risk by solving the empirical risk minimization\n",
      "problem:\n",
      "min\n",
      "¯p∈¯P∥ϕ¯p−y∥2\n",
      "m.\n",
      "Since the set Φ¯Pof shallow ReLU networks is non-convex, this minimization cannot be solved\n",
      "with efficient convex optimizers. Therefore, instead of analyzing only the solution ϕ∗\n",
      "¯pof the opti-\n",
      "mization problem, we introduce a tolerance ϵ >0for the empirical risk and provide bounds on the\n",
      "generalization error, which hold uniformly on the sublevel set\n",
      "¯Qy,ϵ:\n",
      "¯p∈¯P:∥ϕ¯p−y∥2\n",
      "m≤ϵ\t\n",
      ".\n",
      "Before considering generic regression problems, we will initially assume the label yto be a neural\n",
      "network itself, parameterized by a tuple p∗within the hypothesis set P. For all (x, y)in the support of\n",
      "µ, we have y=ϕp∗(x)and the expected risk’s minimum on Pis zero. Using the sufficient condition\n",
      "for NeuRIPs from Theorem 1, we can provide generalization bounds for ϕ¯p∈¯Qy,ϵfor any ϵ >0.\n",
      "Theorem 2. Let¯Pbe a parameter set that satisfies Assumption 1 and let u≥2andt≥ϵ >0be\n",
      "constants. Furthermore, let the number mof samples satisfy\n",
      "m≥8n3c2\n",
      "w(8cb+d+ ln(2)) max\u0012\n",
      "C1u\n",
      "(t−ϵ)2, C2n2c2\n",
      "wu\n",
      "(t−ϵ)2\u0013\n",
      ",\n",
      "where C1andC2are universal constants. Let {(xj, yj)}m\n",
      "j=1be a dataset respecting Assumption 2\n",
      "and let there exist a ¯p∗∈¯Psuch that yj=ϕ¯p∗(xj)holds for all j∈[m]. Then, with probability at\n",
      "least1−17 exp( −u/4), we have for all ¯q∈¯Qy,ϵthat\n",
      "∥ϕ¯q−ϕ¯p∗∥2\n",
      "µ≤t.\n",
      "Proof. We notice that ¯Qy,ϵis a set of shallow neural networks with 2nneurons. We normalize such\n",
      "networks with a function norm greater than tand parameterize them by\n",
      "¯Rt:={ϕ¯p−ϕ¯p∗: ¯p∈¯P,∥ϕ¯p−ϕ¯p∗∥µ> t}.\n",
      "We assume that NeuRIPs( ¯Rt) holds for s= (t−ϵ)2/t2. In this case, for all ¯q∈¯Qy,ϵ, we have that\n",
      "∥ϕ¯q−ϕ¯p∗∥m≥tand thus ¯q /∈¯Qϕ¯p∗,ϵ, which implies that ∥ϕ¯q−ϕ¯p∗∥µ≤t.\n",
      "We also note that ¯Rtsatisfies Assumption 1 with a rescaled constant cw/tand normalization-invariant\n",
      "cb, if¯Psatisfies it for cwandcb. Theorem 1 gives a lower bound on the sample complexity for\n",
      "NeuRIPs( ¯Rt), completing the proof.\n",
      "At any network where an optimization method terminates, the concentration of the empirical risk\n",
      "at the expected risk can be achieved with less data than needed to achieve an analogous NeuRIPs\n",
      "event. However, in the chosen stochastic setting, we cannot assume that the termination of an\n",
      "optimization and the norm concentration at that network are independent events. We overcome this\n",
      "by not specifying the outcome of an optimization method and instead stating uniform bounds on\n",
      "the norm concentration. The only assumption on an algorithm is therefore the identification of a\n",
      "network that permits an upper bound ϵon its empirical risk. The event NeuRIPs( ¯Rt) then restricts the\n",
      "expected risk to be below the corresponding level t.\n",
      "We now discuss the empirical risk surface for generic distributions µthat satisfy Assumption 2, where\n",
      "ydoes not necessarily have to be a neural network.\n",
      "Theorem 3. There exist constants C0,C1,C2,C3,C4, and C5such that the following holds: Let ¯P\n",
      "satisfy Assumption 1 for some constants cw,cb, and let ¯p∗∈¯Pbe such that for some c¯p∗≥0we\n",
      "have\n",
      "Eµ\u0014\n",
      "exp\u0012(y−ϕ¯p∗(x))2\n",
      "c2\n",
      "¯p∗\u0013\u0015\n",
      "≤2.\n",
      "We assume, for any s∈(0,1)and confidence parameter u >0, that the number of samples mis\n",
      "large enough such that\n",
      "m≥8\n",
      "(1−s)2max\u0012\n",
      "C1\u0012n3c2\n",
      "w(8cb+d+ ln(2))\n",
      "u\u0013\n",
      ", C2n2c2\n",
      "w\u0010u\n",
      "s\u0011\u0013\n",
      ".\n",
      "4\n",
      "We further select confidence parameters v1, v2> C 0, and define for some ω≥0the parameter\n",
      "η:= 2(1 −s)∥ϕ¯p∗−y∥µ+C3v1v2c¯p∗1\n",
      "(1−s)1/4+ω√\n",
      "1−s.\n",
      "If we set ϵ=∥ϕ¯p∗−y∥2\n",
      "m+ω2as the tolerance for the empirical risk, then the probability that all\n",
      "¯q∈¯Qy,ϵsatisfy\n",
      "∥ϕ¯q−y∥µ≤η\n",
      "is at least\n",
      "1−17 exp\u0010\n",
      "−u\n",
      "4\u0011\n",
      "−C5v2exp\u0012\n",
      "−C4mv2\n",
      "2\n",
      "2\u0013\n",
      ".\n",
      "Proof sketch. (Complete proof in Appendix E) We first define and decompose the excess risk by\n",
      "E(¯q,¯p∗) :=∥ϕ¯q−y∥2\n",
      "µ− ∥ϕ¯p∗−y∥2\n",
      "µ=∥ϕ¯q−ϕ¯p∗∥2\n",
      "µ−2\n",
      "mmX\n",
      "j=1(ϕ¯p∗(xj)−yj)(ϕ¯q(xj)−ϕ¯p∗(xj)).\n",
      "It suffices to show, that within the stated confidence level we have ∥ϕ¯q−y∥µ> η. This implies the\n",
      "claim since ∥ϕ¯q−y∥m≤ϵimplies ∥ϕ¯q−y∥µ≤η. We have E[E(¯q,¯p∗)]>0. It now only remains\n",
      "to strengthen the condition on η >3∥ϕ¯p∗−y∥µto achieve E(¯q,¯p∗)> ω2. We apply Theorem 1\n",
      "to derive a bound on the fluctuation of the first term. The concentration rate of the second term is\n",
      "derived similar to Theorem 1 by using chaining techniques. Finally in Appendix E, Theorem 12 gives\n",
      "a general bound to achieve\n",
      "E(¯q,¯p∗)> ω2\n",
      "uniformly for all ¯qwith∥ϕ¯q−ϕ¯p∗∥µ> η. Theorem 3 then follows as a simplification.\n",
      "It is important to notice that, in Theorem 3, as the data size mapproaches infinity, one can select\n",
      "an asymptotically small deviation constant s. In this limit, the bound ηon the generalization error\n",
      "converges to 3∥ϕ¯p∗−y∥µ+ω. This reflects a lower limit of the generalization bound, which is the\n",
      "sum of the theoretically achievable minimum of the expected risk and the additional tolerance ω.\n",
      "The latter is an upper bound on the empirical risk, which real-world optimization algorithms can be\n",
      "expected to achieve.\n",
      "5 Size Control of Stochastic Processes on Shallow Networks\n",
      "In this section, we introduce the key techniques for deriving concentration statements for the em-\n",
      "pirical norm, uniformly valid for sets of shallow ReLU networks. We begin by rewriting the event\n",
      "NeuRIPs( ¯P) by treating µas a stochastic process, indexed by the parameter set ¯P. The event\n",
      "NeuRIPs( ¯P) holds if and only if we have\n",
      "sup\n",
      "¯p∈¯P|∥ϕ¯p∥m− ∥ϕ¯p∥µ| ≤ssup\n",
      "¯p∈¯P∥ϕ¯p∥µ.\n",
      "The supremum of stochastic processes has been studied in terms of their size. To determine the size\n",
      "of a process, it is essential to determine the correlation between its variables. To this end, we define\n",
      "the Sub-Gaussian metric for any parameter tuples ¯p,¯q∈¯Pas\n",
      "dψ2(ϕ¯p, ϕ¯q) := inf(\n",
      "Cψ2≥0 :E\"\n",
      "exp \n",
      "|ϕ¯p(x)−ϕ¯q(x)|2\n",
      "C2\n",
      "ψ2!#\n",
      "≤2)\n",
      ".\n",
      "A small Sub-Gaussian metric between random variables indicates that their values are likely to be\n",
      "close. To capture the Sub-Gaussian structure of a process, we introduce ϵ-nets in the Sub-Gaussian\n",
      "metric. For a given ϵ >0, these are subsets ¯Q⊆¯Psuch that for every ¯p∈¯P, there is a ¯q∈¯Q\n",
      "satisfying\n",
      "dψ2(ϕ¯p, ϕ¯q)≤ϵ.\n",
      "The smallest cardinality of such an ϵ-net ¯Qis known as the Sub-Gaussian covering number\n",
      "N(Φ¯P, dψ2, ϵ). The next Lemma offers a bound for such covering numbers specific to shallow\n",
      "ReLU networks.\n",
      "5\n",
      "Lemma 1. Let¯Pbe a parameter set satisfying Assumption 1. Then there exists a set ˆPwith ¯P⊆ˆP\n",
      "such that\n",
      "N(ΦˆP, dψ2, ϵ)≤2n·\u001216ncbcw\n",
      "ϵ+ 1\u0013n\n",
      "·\u001232ncbcw\n",
      "ϵ+ 1\u0013n\n",
      "·\u00121\n",
      "ϵsin\u00121\n",
      "16ncw\u0013\n",
      "+ 1\u0013d\n",
      ".\n",
      "The proof of this Lemma is based on the theory of stochastic processes and can be seen in Theorem 8\n",
      "of Appendix C.\n",
      "To obtain bounds of the form (6) on the size of a process, we use the generic chaining method. This\n",
      "method offers bounds in terms of the Talagrand-functional of the process in the Sub-Gaussian metric.\n",
      "We define it as follows. A sequence T= (Tk)k∈N0in a set Tis admissible if T0= 1andTk≤2(2k).\n",
      "The Talagrand-functional of the metric space is then defined as\n",
      "γ2(T, d) := inf\n",
      "(Tk)sup\n",
      "t∈T∞X\n",
      "k=02kd(t, Tk),\n",
      "where the infimum is taken across all admissible sequences.\n",
      "With the bounds on the Sub-Gaussian covering number from Lemma 1, we provide a bound on the\n",
      "Talagrand-functional for shallow ReLU networks in the following Lemma. This bound is expected to\n",
      "be of independent interest.\n",
      "Lemma 2. Let¯Psatisfy Assumption 1. Then we have\n",
      "γ2(Φ¯P, dψ2)≤r\n",
      "2\n",
      "π\u00128n3/2cw(8cb+d+ 1)\n",
      "ln(2)p\n",
      "2 ln(2)\u0013\n",
      ".\n",
      "The key ideas to show this bound are similar to the ones used to prove Theorem 9 in Appendix C.\n",
      "To provide bounds for the empirical process, we use the following Lemma, which we prove in\n",
      "Appendix D.\n",
      "Lemma 3. LetΦbe a set of real functions, indexed by a parameter set ¯Pand define\n",
      "N(Φ) :=Z∞\n",
      "0q\n",
      "lnN(Φ, dψ2, ϵ)dϵ and ∆(Φ) := sup\n",
      "ϕ∈Φ∥ϕ∥ψ2.\n",
      "Then, for any u≥2, we have with probability at least 1−17 exp( −u/4)that\n",
      "sup\n",
      "ϕ∈Φ|∥ϕ∥m− ∥ϕ∥µ| ≤u√m\u0014\n",
      "N(Φ) +10\n",
      "3∆(Φ)\u0015\n",
      ".\n",
      "The bounds on the sample complexity for achieving the NeuRIPs event, from Theorem 1, are proven\n",
      "by applying these Lemmata.\n",
      "Proof of Theorem 1. Since we assume ||ϕ¯p||µ>1for all ¯p∈¯P, we have\n",
      "sup\n",
      "¯p∈¯P|∥ϕ¯p∥m− ∥ϕ¯p∥µ| ≤sup\n",
      "¯p∈¯P|∥ϕ¯p∥m− ∥ϕ¯p∥µ|/∥ϕ¯p∥µ.\n",
      "Applying Lemma 3, and further applying the bounds on the covering numbers and the Talagrand-\n",
      "functional for shallow ReLU networks, the NeuRIPs( ¯P) event holds in case of s >3. The sample\n",
      "complexities that are provided in Theorem 1 follow from a refinement of this condition.\n",
      "6 Uniform Generalization of Sublevel Sets of the Empirical Risk\n",
      "In case of the NeuRIPs event, the function norm || · || µcorresponding to the expected risk is close\n",
      "to|| · || m, which corresponds to the empirical risk. With the previous results, we can now derive\n",
      "uniform generalization error bounds in the sublevel set of the empirical risk.\n",
      "We use similar techniques and we define the following sets.\n",
      "∥f∥p= sup\n",
      "1≤q≤p∥f∥q\n",
      "Λk0,u= inf\n",
      "(Tk)sup\n",
      "f∈F∞X\n",
      "k02k∥f−Tk(f)∥u2k\n",
      "6\n",
      "and we need the following lemma:\n",
      "Lemma 9. For any set Fof functions and u≥1, we have\n",
      "Λ0,u(F)≤2√e(γ2(F, dψ2) + ∆( F)).\n",
      "Theorem 10. Let P be a parameter set satisfying Assumption 1. Then, for any u≥1, we have with\n",
      "probability at least 1−17 exp( −u/4)that\n",
      "sup\n",
      "¯p∈P∥ϕ¯p∥m− ∥ϕ¯p∥µ≤u√m\u0010\n",
      "16n3/2cw(8cb+d+ 1) + 2 ncw\u0011\n",
      ".\n",
      "Proof. To this end we have to bound the Talagrand functional, where we can use Dudley’s inequality\n",
      "(Lemma 6). To finish the proof, we apply the bounds on the covering numbers provided by Theorem\n",
      "6.\n",
      "Theorem 11. Let¯P⊆(Rd×R× ±1)nsatisfy Assumption 1. Then there exist universal constants\n",
      "C1,C2such that\n",
      "sup\n",
      "¯p∈P∥ϕ¯p∥m− ∥ϕ¯p∥µ≤r\n",
      "2\n",
      "π\u00128n3/2cw(8cb+d+ 1)\n",
      "ln(2)p\n",
      "2 ln(2)\u0013\n",
      ".\n",
      "7 Conclusion\n",
      "In this study, we investigated the empirical risk surface of shallow ReLU networks in terms of uniform\n",
      "concentration events for the empirical norm. We defined the Neural Restricted Isometry Property\n",
      "(NeuRIPs) and determined the sample complexity required to achieve NeuRIPs, which depends on\n",
      "realistic parameter bounds and the network architecture. We applied our findings to derive upper\n",
      "bounds on the expected risk, which are valid uniformly across sublevel sets of the empirical risk.\n",
      "If a network optimization algorithm can identify a network with a small empirical risk, our results\n",
      "guarantee that this network will generalize well. By deriving uniform concentration statements, we\n",
      "have resolved the problem of independence between the termination of an optimization algorithm at\n",
      "a certain network and the empirical risk concentration at that network. Future studies may focus on\n",
      "performing uniform empirical norm concentration on the critical points of the empirical risk, which\n",
      "could lead to even tighter bounds for the sample complexity.\n",
      "We also plan to apply our methods to input distributions more general than the Gaussian distribution.\n",
      "If generic Gaussian distributions can be handled, one could then derive bounds for the Sub-Gaussian\n",
      "covering number for deep ReLU networks by induction across layers. We also expect that our\n",
      "results on the covering numbers could be extended to more generic Lipschitz continuous activation\n",
      "functions other than ReLU. This proposition is based on the concentration of measure phenomenon,\n",
      "which provides bounds on the Sub-Gaussian norm of functions on normal concentrating input spaces.\n",
      "Because these bounds scale with the Lipschitz constant of the function, they can be used to find ϵ-nets\n",
      "for neurons that have identical activation patterns.\n",
      "Broader Impact\n",
      "Supervised machine learning now affects both personal and public lives significantly. Generalization is\n",
      "critical to the reliability and safety of empirically trained models. Our analysis aims to achieve a deeper\n",
      "understanding of the relationships between generalization, architectural design, and available data.\n",
      "We have discussed the concepts and demonstrated the effectiveness of using uniform concentration\n",
      "events for generalization guarantees of common supervised machine learning algorithms.\n",
      "7\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\vikra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Function to validate paragraphs\n",
    "def validate_paragraphs(paragraphs, word_threshold=50):\n",
    "    results = []\n",
    "    for para in paragraphs:\n",
    "        sentences = sent_tokenize(para)  # Tokenize into sentences\n",
    "        if len(sentences) == 0:  # No sentences detected\n",
    "            results.append((para, False))  # Invalid paragraph\n",
    "            continue\n",
    "        avg_words_per_sentence = len(para.split()) / len(sentences)\n",
    "        results.append((para, avg_words_per_sentence <= word_threshold))\n",
    "    return results\n",
    "\n",
    "# Split the extracted text into paragraphs\n",
    "paragraphs = pdf_text.split(\"\\n\\n\")  # Assuming double newlines separate paragraphs\n",
    "validation_results = validate_paragraphs(paragraphs)\n",
    "\n",
    "# Display validation results\n",
    "for para, is_valid in validation_results:\n",
    "    print(f\"VALID: {is_valid}\\nParagraph: {para}...\\n\")  # Print first 200 chars for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title and Abstract Misalignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title and Abstract are misaligned (Similarity Score: 0.48)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load pre-trained model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Check title-abstract alignment\n",
    "def check_title_abstract_alignment(title, abstract, threshold=0.5):\n",
    "    title_embedding = model.encode(title, convert_to_tensor=True)\n",
    "    abstract_embedding = model.encode(abstract, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(title_embedding, abstract_embedding)\n",
    "    return similarity.item() >= threshold, similarity.item()\n",
    "\n",
    "# Perform the check\n",
    "is_aligned, similarity_score = check_title_abstract_alignment(title, abstract)\n",
    "print(f\"Title and Abstract are {'aligned' if is_aligned else 'misaligned'} (Similarity Score: {similarity_score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstracts having Multiple Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "paragraphs = abstract.strip().split(\"\\n\\n\")\n",
    "multiple_paras = len(paragraphs) > 1\n",
    "print(multiple_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
